{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retail Returns Data Analysis Project\n",
    "For a dataset generated by Gurudatt Bhobe, founder of supply.ai.\n",
    "\n",
    "The tools used for this project were pandas, numpy, matplotlib.pyplot and sklearn.\n",
    "\n",
    "This notebook is separated into 6 parts that show my thought process and final results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Part 1*\n",
    "I first viewed the original Dataset in MS Excel to analyze what features were given and their datatypes. Of the original number of features (103), roughly half of them were very, very likely to be useless or extremely hard to make use of. This is broken down in the large multi-line comment above the list 'unhelpfulFeatures.' I removed these before proceeding. I then took some of the useful features that could be converted to numerical format and made those changes. After that, I separated all of the quantitative features into a separate fram 'quantFrame' in order to perform some basic statistical analysis (pairwise correlation, predictivness). This was primarily to weed out features that were highly correlated and to see what features, if any, seemed to predict the outcome well. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelkant/anaconda3/lib/python3.4/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  4.40000000e+01,   9.00000000e+00,   1.40000000e+01,\n",
       "         2.40000000e+01,   3.40000000e+01,   4.20000000e+01,\n",
       "         1.99940186e+03,   1.99922051e+04,   1.00000000e+00,\n",
       "         3.40000000e+01,   1.00000000e+00,   4.38445703e+03,\n",
       "         2.90000000e+01,   1.99843685e+02,   9.94914613e+00,\n",
       "         1.49996143e+01,   4.39045605e+03,   9.00000000e+00,\n",
       "         1.99912811e+02,   1.99890022e+01,   9.99185562e+00,\n",
       "         1.99933434e+01,   6.99000000e+02,   1.00000000e+00,\n",
       "         9.99984622e-01,   1.00000000e+00,   2.00000000e+00,\n",
       "         4.00000000e+00,   4.50000000e+00,   1.00000000e+00,\n",
       "         2.00000000e+00,   1.00000000e+00,   3.00000000e+00])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# load up half of the dataset to be the training set (accidentally picked the second half but whatever)\n",
    "trainFrame = pd.read_csv(\"Dataset.csv\").loc[50000:]\n",
    "\n",
    "\"\"\"\n",
    "Going to remove colummns that are unhelpful for various reasons: \n",
    "    - any features that are simply hexadecimal hashed identifiers are impossible to use in ML classification\n",
    "    - any location based features are impossible to generalize (test set may have locations never seen before)\n",
    "    - some timestamps are rendered unhelpful because of more helpful timestamps in the data\n",
    "    - all features that are given values only if something is returned wouldn't be present for test set examples\n",
    "    - some features are blank for all entries, some are only one value for all entries. \"\"\"\n",
    "\n",
    "unhelpfulFeatures = ['user_id', 'user_street_address', 'user_zip_code', 'user_city', 'order_id', \n",
    "'order_line_id', 'return_order_line_id_x', 'product_id', 'shipping_id', 'Unnamed: 0', \n",
    "'location_id', 'merchant_id', 'transaction_id', 'transaction_timestamp', 'transaction_ip', \n",
    "'transaction_comments', 'product_name', 'airway_bill_number', 'courier_id', 'product_description', \n",
    "'dispatch_zipcode', 'dispatch_city', 'transaction_status', 'current_status', 'user_review', \n",
    "'destination_zipcode', 'destination_city', 'dispatch_date', 'last_mile_arrival_date',\n",
    "'location_zip_code', 'location_city', 'location_street_address', 'overpurchase_flag',\n",
    "'splitshipment_flag', 'return_order_line_id_y', 'return_date', 'return_cause', 'action_taken', \n",
    "'product_return_score', 'restocking_cost', 'reverse_shipping_cost', 'overall_return_score', \n",
    "'transaction_score', 'user_return_score', 'id', 'size_features']\n",
    "\n",
    "skimFrame = trainFrame.drop(unhelpfulFeatures, axis=1)\n",
    "\n",
    "# useful to look at in excel so I'll keep it handy\n",
    "skimFrame.to_csv(\"skimDataset.csv\")\n",
    "\n",
    "# convert the two gendered features into zero-one features (male = 1, female = 0)\n",
    "gender1 = lambda x : 1 if x == \"m\" else 0\n",
    "gender2 = lambda x : 1 if x == \"Men\" else 0\n",
    "\n",
    "skimFrame.loc[:, 'user_gender'] = skimFrame['user_gender'].apply(gender1)\n",
    "skimFrame.loc[:, 'product_attribute_gender'] = skimFrame['product_attribute_gender'].apply(gender2)\n",
    "\n",
    "# convert the one feature that has boolean values into zero-one\n",
    "skimFrame.loc[:, 'fit_flag'] = skimFrame['fit_flag'].astype(int)\n",
    "\n",
    "# create a new feature -> abs(fit_score) + fit_flag since fit_score seems to just be how off the fit is from ideal.\n",
    "# Sometimes fit_score = 0 and fit_flag is True. We account for this by adding in the fit_flag value too. \n",
    "absolute = lambda x: abs(x)\n",
    "fit_score2 = skimFrame['fit_score'].apply(absolute) + skimFrame['fit_flag']\n",
    "skimFrame.loc[:, 'fit_score2'] = fit_score2\n",
    "\n",
    "# begin by isolating quantitative data away from strings\n",
    "# of the 59 features, 33 of them are integers and floats.\n",
    "quantFrame = skimFrame.select_dtypes(exclude=[\"object\"])\n",
    "\n",
    "# scale all quantitative data to values between zero and one.\n",
    "dataTypes = quantFrame.dtypes\n",
    "quantFrameMaxValues = np.ones(dataTypes.size)\n",
    "for i in range(dataTypes.size):\n",
    "    feature = quantFrame.axes[1][i]\n",
    "    maxValue = quantFrame[feature].max()\n",
    "    if maxValue != 1:\n",
    "        quantFrameMaxValues[i] = maxValue\n",
    "        quantFrame.loc[:,feature] = quantFrame[feature] / float(maxValue)\n",
    "\n",
    "# compute pairwise correlations between different features\n",
    "pairCorrelations = quantFrame.corr()\n",
    "for i in range(dataTypes.size):\n",
    "    pairCorrelations.iloc[i,i] = 0.0\n",
    "    \n",
    "quantFrameMaxValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlations with return flag</th>\n",
       "      <th>maximum correlation</th>\n",
       "      <th>miminum correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_visits</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_orders</th>\n",
       "      <td>-0.003131</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>-0.043536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visits_in_last_week</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visits_in_last_month</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visits_in_last_quarter</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>visits_in_last_year</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_purchase_value</th>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>-0.026826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_purchase_value</th>\n",
       "      <td>-0.008687</td>\n",
       "      <td>0.867943</td>\n",
       "      <td>-0.042715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_value_customer</th>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.867943</td>\n",
       "      <td>-0.037513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_age</th>\n",
       "      <td>-0.003300</td>\n",
       "      <td>0.027987</td>\n",
       "      <td>-0.031086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_gender</th>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.032131</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_order_value</th>\n",
       "      <td>0.091256</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.031086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_item_count</th>\n",
       "      <td>0.097377</td>\n",
       "      <td>0.948345</td>\n",
       "      <td>-0.029058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_price</th>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>-0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_discount</th>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.616540</td>\n",
       "      <td>-0.022072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipping_fees</th>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>-0.009005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_value</th>\n",
       "      <td>0.091247</td>\n",
       "      <td>0.999997</td>\n",
       "      <td>-0.031035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_fees</th>\n",
       "      <td>-0.005634</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>-0.025319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_list_price</th>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>-0.030201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_length</th>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.025595</td>\n",
       "      <td>-0.108221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_breadth</th>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>-0.012873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_height</th>\n",
       "      <td>-0.002032</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>-0.015954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_weight</th>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.720316</td>\n",
       "      <td>-0.108221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_attribute_gender</th>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.074896</td>\n",
       "      <td>-0.030201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipment_weight</th>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>-0.009499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.720316</td>\n",
       "      <td>-0.080583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivery_attempt_count</th>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>-0.011368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_rating</th>\n",
       "      <td>-0.002437</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>-0.007721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_review_sentiment_score</th>\n",
       "      <td>-0.001909</td>\n",
       "      <td>0.008856</td>\n",
       "      <td>-0.004937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_flag</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117040</td>\n",
       "      <td>-0.009513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_score</th>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>-0.009062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_flag</th>\n",
       "      <td>0.117040</td>\n",
       "      <td>0.901817</td>\n",
       "      <td>-0.006394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_score2</th>\n",
       "      <td>0.107626</td>\n",
       "      <td>0.901817</td>\n",
       "      <td>-0.008737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             correlations with return flag  \\\n",
       "total_visits                                      0.000038   \n",
       "total_orders                                     -0.003131   \n",
       "visits_in_last_week                               0.000038   \n",
       "visits_in_last_month                              0.000038   \n",
       "visits_in_last_quarter                            0.000038   \n",
       "visits_in_last_year                               0.000038   \n",
       "last_purchase_value                               0.000904   \n",
       "total_purchase_value                             -0.008687   \n",
       "high_value_customer                              -0.009513   \n",
       "user_age                                         -0.003300   \n",
       "user_gender                                       0.011973   \n",
       "total_order_value                                 0.091256   \n",
       "total_item_count                                  0.097377   \n",
       "product_price                                     0.006276   \n",
       "product_discount                                  0.001911   \n",
       "shipping_fees                                     0.003596   \n",
       "transaction_value                                 0.091247   \n",
       "transaction_fees                                 -0.005634   \n",
       "product_list_price                                0.006200   \n",
       "product_length                                    0.009091   \n",
       "product_breadth                                   0.012043   \n",
       "product_height                                   -0.002032   \n",
       "product_weight                                    0.000578   \n",
       "product_attribute_gender                          0.002596   \n",
       "shipment_weight                                   0.004994   \n",
       "heavy                                            -0.000184   \n",
       "delivery_attempt_count                            0.004741   \n",
       "user_rating                                      -0.002437   \n",
       "user_review_sentiment_score                      -0.001909   \n",
       "return_flag                                       0.000000   \n",
       "fit_score                                         0.002104   \n",
       "fit_flag                                          0.117040   \n",
       "fit_score2                                        0.107626   \n",
       "\n",
       "                             maximum correlation  miminum correlation  \n",
       "total_visits                            1.000000            -0.090821  \n",
       "total_orders                            0.025679            -0.043536  \n",
       "visits_in_last_week                     1.000000            -0.090821  \n",
       "visits_in_last_month                    1.000000            -0.090821  \n",
       "visits_in_last_quarter                  1.000000            -0.090821  \n",
       "visits_in_last_year                     1.000000            -0.090821  \n",
       "last_purchase_value                     0.025679            -0.026826  \n",
       "total_purchase_value                    0.867943            -0.042715  \n",
       "high_value_customer                     0.867943            -0.037513  \n",
       "user_age                                0.027987            -0.031086  \n",
       "user_gender                             0.032131            -0.090821  \n",
       "total_order_value                       0.999997            -0.031086  \n",
       "total_item_count                        0.948345            -0.029058  \n",
       "product_price                           0.999466            -0.030042  \n",
       "product_discount                        0.616540            -0.022072  \n",
       "shipping_fees                           0.010101            -0.009005  \n",
       "transaction_value                       0.999997            -0.031035  \n",
       "transaction_fees                        0.020842            -0.025319  \n",
       "product_list_price                      0.999466            -0.030201  \n",
       "product_length                          0.025595            -0.108221  \n",
       "product_breadth                         0.051459            -0.012873  \n",
       "product_height                          0.047872            -0.015954  \n",
       "product_weight                          0.720316            -0.108221  \n",
       "product_attribute_gender                0.074896            -0.030201  \n",
       "shipment_weight                         0.005937            -0.009499  \n",
       "heavy                                   0.720316            -0.080583  \n",
       "delivery_attempt_count                  0.008907            -0.011368  \n",
       "user_rating                             0.008907            -0.007721  \n",
       "user_review_sentiment_score             0.008856            -0.004937  \n",
       "return_flag                             0.117040            -0.009513  \n",
       "fit_score                               0.011436            -0.009062  \n",
       "fit_flag                                0.901817            -0.006394  \n",
       "fit_score2                              0.901817            -0.008737  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answers to some burning questions\n",
    "relevantCorrelations = pd.DataFrame(data={'miminum correlation' : pairCorrelations.min(), \n",
    "                                          'maximum correlation' : pairCorrelations.max(),\n",
    "                                          'correlations with return flag' : pairCorrelations['return_flag']})\n",
    "relevantCorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made it this far\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlations with return flag</th>\n",
       "      <th>maximum correlation</th>\n",
       "      <th>means</th>\n",
       "      <th>medians</th>\n",
       "      <th>miminum correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_visits</th>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.955265</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_orders</th>\n",
       "      <td>-0.003131</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.511258</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.043536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_purchase_value</th>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.507261</td>\n",
       "      <td>0.522240</td>\n",
       "      <td>-0.026826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high_value_customer</th>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.012135</td>\n",
       "      <td>0.487060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.037513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_age</th>\n",
       "      <td>-0.003300</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>0.760759</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>-0.031086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_gender</th>\n",
       "      <td>0.011973</td>\n",
       "      <td>0.032131</td>\n",
       "      <td>0.509800</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.090821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_order_value</th>\n",
       "      <td>0.091256</td>\n",
       "      <td>0.948313</td>\n",
       "      <td>0.384500</td>\n",
       "      <td>0.408427</td>\n",
       "      <td>-0.031086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_item_count</th>\n",
       "      <td>0.097377</td>\n",
       "      <td>0.948313</td>\n",
       "      <td>0.522757</td>\n",
       "      <td>0.586207</td>\n",
       "      <td>-0.029058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_price</th>\n",
       "      <td>0.006276</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.518487</td>\n",
       "      <td>0.521910</td>\n",
       "      <td>-0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_discount</th>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.590476</td>\n",
       "      <td>0.267426</td>\n",
       "      <td>0.212903</td>\n",
       "      <td>-0.022072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipping_fees</th>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.500075</td>\n",
       "      <td>0.501732</td>\n",
       "      <td>-0.009005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transaction_fees</th>\n",
       "      <td>-0.005634</td>\n",
       "      <td>0.020842</td>\n",
       "      <td>0.497396</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.025319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_length</th>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.025516</td>\n",
       "      <td>0.742742</td>\n",
       "      <td>0.742358</td>\n",
       "      <td>-0.108221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_breadth</th>\n",
       "      <td>0.012043</td>\n",
       "      <td>0.051459</td>\n",
       "      <td>0.743754</td>\n",
       "      <td>0.745202</td>\n",
       "      <td>-0.012873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_height</th>\n",
       "      <td>-0.002032</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>0.765191</td>\n",
       "      <td>0.772121</td>\n",
       "      <td>-0.015954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_weight</th>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.720316</td>\n",
       "      <td>0.852602</td>\n",
       "      <td>0.851216</td>\n",
       "      <td>-0.108221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_attribute_gender</th>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.074896</td>\n",
       "      <td>0.485340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shipment_weight</th>\n",
       "      <td>0.004994</td>\n",
       "      <td>0.005937</td>\n",
       "      <td>0.497745</td>\n",
       "      <td>0.495568</td>\n",
       "      <td>-0.009499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heavy</th>\n",
       "      <td>-0.000184</td>\n",
       "      <td>0.720316</td>\n",
       "      <td>0.216700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.080583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivery_attempt_count</th>\n",
       "      <td>0.004741</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.499660</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.011368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_rating</th>\n",
       "      <td>-0.002437</td>\n",
       "      <td>0.008907</td>\n",
       "      <td>0.497825</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.007721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_review_sentiment_score</th>\n",
       "      <td>-0.001909</td>\n",
       "      <td>0.008856</td>\n",
       "      <td>0.500224</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.004937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_flag</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117040</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.009513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_flag</th>\n",
       "      <td>0.117040</td>\n",
       "      <td>0.901817</td>\n",
       "      <td>0.503480</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.006394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fit_score2</th>\n",
       "      <td>0.107626</td>\n",
       "      <td>0.901817</td>\n",
       "      <td>0.370740</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.008737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             correlations with return flag  \\\n",
       "total_visits                                      0.000038   \n",
       "total_orders                                     -0.003131   \n",
       "last_purchase_value                               0.000904   \n",
       "high_value_customer                              -0.009513   \n",
       "user_age                                         -0.003300   \n",
       "user_gender                                       0.011973   \n",
       "total_order_value                                 0.091256   \n",
       "total_item_count                                  0.097377   \n",
       "product_price                                     0.006276   \n",
       "product_discount                                  0.001911   \n",
       "shipping_fees                                     0.003596   \n",
       "transaction_fees                                 -0.005634   \n",
       "product_length                                    0.009091   \n",
       "product_breadth                                   0.012043   \n",
       "product_height                                   -0.002032   \n",
       "product_weight                                    0.000578   \n",
       "product_attribute_gender                          0.002596   \n",
       "shipment_weight                                   0.004994   \n",
       "heavy                                            -0.000184   \n",
       "delivery_attempt_count                            0.004741   \n",
       "user_rating                                      -0.002437   \n",
       "user_review_sentiment_score                      -0.001909   \n",
       "return_flag                                       0.000000   \n",
       "fit_flag                                          0.117040   \n",
       "fit_score2                                        0.107626   \n",
       "\n",
       "                             maximum correlation     means   medians  \\\n",
       "total_visits                            0.010101  0.955265  0.954545   \n",
       "total_orders                            0.025679  0.511258  0.555556   \n",
       "last_purchase_value                     0.025679  0.507261  0.522240   \n",
       "high_value_customer                     0.012135  0.487060  0.000000   \n",
       "user_age                                0.020842  0.760759  0.764706   \n",
       "user_gender                             0.032131  0.509800  1.000000   \n",
       "total_order_value                       0.948313  0.384500  0.408427   \n",
       "total_item_count                        0.948313  0.522757  0.586207   \n",
       "product_price                           0.590476  0.518487  0.521910   \n",
       "product_discount                        0.590476  0.267426  0.212903   \n",
       "shipping_fees                           0.010101  0.500075  0.501732   \n",
       "transaction_fees                        0.020842  0.497396  0.444444   \n",
       "product_length                          0.025516  0.742742  0.742358   \n",
       "product_breadth                         0.051459  0.743754  0.745202   \n",
       "product_height                          0.047872  0.765191  0.772121   \n",
       "product_weight                          0.720316  0.852602  0.851216   \n",
       "product_attribute_gender                0.074896  0.485340  0.000000   \n",
       "shipment_weight                         0.005937  0.497745  0.495568   \n",
       "heavy                                   0.720316  0.216700  0.000000   \n",
       "delivery_attempt_count                  0.008907  0.499660  0.500000   \n",
       "user_rating                             0.008907  0.497825  0.500000   \n",
       "user_review_sentiment_score             0.008856  0.500224  0.555556   \n",
       "return_flag                             0.117040  0.013700  0.000000   \n",
       "fit_flag                                0.901817  0.503480  1.000000   \n",
       "fit_score2                              0.901817  0.370740  0.333333   \n",
       "\n",
       "                             miminum correlation  \n",
       "total_visits                           -0.090821  \n",
       "total_orders                           -0.043536  \n",
       "last_purchase_value                    -0.026826  \n",
       "high_value_customer                    -0.037513  \n",
       "user_age                               -0.031086  \n",
       "user_gender                            -0.090821  \n",
       "total_order_value                      -0.031086  \n",
       "total_item_count                       -0.029058  \n",
       "product_price                          -0.030042  \n",
       "product_discount                       -0.022072  \n",
       "shipping_fees                          -0.009005  \n",
       "transaction_fees                       -0.025319  \n",
       "product_length                         -0.108221  \n",
       "product_breadth                        -0.012873  \n",
       "product_height                         -0.015954  \n",
       "product_weight                         -0.108221  \n",
       "product_attribute_gender               -0.030042  \n",
       "shipment_weight                        -0.009499  \n",
       "heavy                                  -0.080583  \n",
       "delivery_attempt_count                 -0.011368  \n",
       "user_rating                            -0.007721  \n",
       "user_review_sentiment_score            -0.004937  \n",
       "return_flag                            -0.009513  \n",
       "fit_flag                               -0.006394  \n",
       "fit_score2                             -0.008737  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "Interestingly, none of the features are significantly negatively correlated with one another. \n",
    "Also, sadly, none of the features are signficantly correlated with return_flag.\n",
    "However, we find that the following sets of features are highly correlated with one another.\n",
    "    - total_visits, visits_in_last_week, visits_in_last_month, visits_in_last_quarter, visits_in_last_year (r = 1)\n",
    "    - total_order_value, transaction_value (r = 0.999997)\n",
    "    - product_price, product_list_price (r = 0.999466)\n",
    "    - fit_flag, fit_score2 (r = 0.901817) but both are relatively predictive so I won't remove either. \n",
    "    - total_purchase_value, high_value_customer (r = 0.867493)\n",
    "From each set we only keep the feature that correlates best with return_flag\n",
    "    - We'll keep fit_score2 and leave out fit_score since it performed better with return_flag \"\"\"\n",
    "\n",
    "redundantFeatures = ['visits_in_last_week', 'visits_in_last_month', 'visits_in_last_quarter', 'fit_score',  \n",
    "                    'visits_in_last_year', 'transaction_value', 'product_list_price', 'total_purchase_value']\n",
    "skimFrame = skimFrame.drop(redundantFeatures, axis=1)\n",
    "quantFrame = quantFrame.drop(redundantFeatures, axis=1)\n",
    "\n",
    "# We'll do this once again to see if more high pairwise correlations are still present.\n",
    "pairCorrelations2 = quantFrame.corr()\n",
    "for i in range(dataTypes.size - len(redundantFeatures)):\n",
    "\tpairCorrelations2.iloc[i,i] = 0.0\n",
    "    \n",
    "print('made it this far')\n",
    "    \n",
    "relevantstats = pd.DataFrame(data={'miminum correlation' : pairCorrelations2.min(), \n",
    "                                          'maximum correlation' : pairCorrelations2.max(),\n",
    "                                          'correlations with return flag' : pairCorrelations2['return_flag'], \n",
    "                                          'means' : quantFrame.mean(axis=0),\n",
    "                                          'medians' : quantFrame.median(axis=0)})\n",
    "relevantstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# total_order_value and total_item_count are highly correlated (r = 0.948313)\n",
    "# I'll still keep them both since they are amongst the most predictive features. \n",
    "# it's not possible for any other correlations to be higher than 0.720316 excluding fit_flag and fit_score2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *** Part 1b ***\n",
    "Here I checked out some of the features to see what exactly the nature of the correlations were. I found some interesting results that helped show that there were definitely some patterns to be revealed in the data. With near certainty we can conclude that any record with fit_score2 = 0, total_item_count <= 15 or total_order_value <= 1000 will not be returned...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fit_score2  return_flag\n",
       "0           0              24826\n",
       "1           0               4801\n",
       "            1                120\n",
       "2           0               9782\n",
       "            1                287\n",
       "3           0               9906\n",
       "            1                278\n",
       "Name: user_signup_date, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skimFrame['user_signup_date'].groupby([skimFrame['fit_score2'], skimFrame['return_flag']]).count()\n",
    "\n",
    "# If I were building a decision tree, this would be the first question/fork. \n",
    "# Clearly, if fit_flag == 0, then return_flag = 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10326e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgkAAAFkCAYAAACq4KjhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9gQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x107c2bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "total_order_value     return_flag\n",
       "(0, 1096.114]         0              17615\n",
       "                      1                  1\n",
       "(1096.114, 2192.229]  0              13391\n",
       "                      1                220\n",
       "(2192.229, 3288.343]  0              14372\n",
       "                      1                360\n",
       "Name: user_signup_date, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.asarray(skimFrame['total_item_count'])\n",
    "x2 = np.asarray(skimFrame['total_order_value'])\n",
    "y = np.asarray(quantFrame['return_flag'])\n",
    "plt.plot(x, y, 'o')\n",
    "plt.axis([0, skimFrame['total_item_count'].max(), -0.5, 1.5])\n",
    "plt.show()\n",
    "# Across 50,000 training examples, returns were only made if total_item_count was greater than 15. \n",
    "# This would be another fork in my decision tree. \n",
    "\n",
    "plt.plot(x2, y, 'ro')\n",
    "plt.axis([0, skimFrame['total_order_value'].max(), -0.5, 1.5])\n",
    "plt.show()\n",
    "# Like 'total_item_count', there appears to be a lower bound (1000) on this feature to even be considered for a return.\n",
    "\n",
    "col = skimFrame[\"total_order_value\"]\n",
    "skimFrame['user_signup_date'].groupby([pd.cut(col, np.arange(0, col.max(), col.max() / 4)), skimFrame['return_flag']]).count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Part 2*\n",
    "After my analysis of the quantitative features (booleans included), I moved onto working with datetime data. I performed the same kind of analysis as I did with the regular quantitative features. I made the assumption that the intrinsic dates were not as important as the spreads between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We're going to look to date features now to find good quantitative features. \n",
    "    - After researching reverse logistics and RTO, I think it's reasonable to exclude these dates from \n",
    "    consideration since if this is realistic, no person would take into account either of these for making returns.\n",
    "    - user_signup_date does not seem to be important when considering if a return is made.\n",
    "    - The dates themselves are not as important as the spreads between them as this is what people actually notice. \n",
    "        + Let's make 'order_created_date' as t = 0 for an order. \"\"\"\n",
    "\n",
    "timeFeatures = ['last_active_date', 'last_purchase_date', 'last_return_date',\n",
    "               'promised_date', 'pickup_confirmation_date', 'delivered_date',\n",
    "               'cancelled_date', 'courier_onboarding_date']\n",
    "\n",
    "scale = lambda x : x.seconds if hasattr(x, \"seconds\") else x # to avoid errors with NaT\n",
    "clean = lambda x : None if x == -9223372036854775808 else x # after changing to int, NaT's become that number\n",
    "\n",
    "for feature in timeFeatures:\n",
    "    newSeries = pd.to_datetime(skimFrame[feature]) - pd.to_datetime(skimFrame['order_created_date'])\n",
    "    skimFrame.loc[:, feature] = newSeries.apply(scale).astype(int).apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max times (since order)</th>\n",
       "      <th>mean times</th>\n",
       "      <th>median times</th>\n",
       "      <th>minimum times</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>last_active_date</th>\n",
       "      <td>61320.0</td>\n",
       "      <td>32327.635200</td>\n",
       "      <td>32460.0</td>\n",
       "      <td>2340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_purchase_date</th>\n",
       "      <td>21540.0</td>\n",
       "      <td>10832.152800</td>\n",
       "      <td>10860.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_return_date</th>\n",
       "      <td>41640.0</td>\n",
       "      <td>21539.517600</td>\n",
       "      <td>21480.0</td>\n",
       "      <td>120.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promised_date</th>\n",
       "      <td>32280.0</td>\n",
       "      <td>16137.837047</td>\n",
       "      <td>16080.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_confirmation_date</th>\n",
       "      <td>3540.0</td>\n",
       "      <td>1771.891307</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivered_date</th>\n",
       "      <td>53820.0</td>\n",
       "      <td>26969.437297</td>\n",
       "      <td>27000.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancelled_date</th>\n",
       "      <td>75000.0</td>\n",
       "      <td>37804.908000</td>\n",
       "      <td>37860.0</td>\n",
       "      <td>360.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>courier_onboarding_date</th>\n",
       "      <td>86340.0</td>\n",
       "      <td>43168.602000</td>\n",
       "      <td>43200.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          max times (since order)    mean times  median times  \\\n",
       "last_active_date                          61320.0  32327.635200       32460.0   \n",
       "last_purchase_date                        21540.0  10832.152800       10860.0   \n",
       "last_return_date                          41640.0  21539.517600       21480.0   \n",
       "promised_date                             32280.0  16137.837047       16080.0   \n",
       "pickup_confirmation_date                   3540.0   1771.891307        1800.0   \n",
       "delivered_date                            53820.0  26969.437297       27000.0   \n",
       "cancelled_date                            75000.0  37804.908000       37860.0   \n",
       "courier_onboarding_date                   86340.0  43168.602000       43200.0   \n",
       "\n",
       "                          minimum times  \n",
       "last_active_date                 2340.0  \n",
       "last_purchase_date                  0.0  \n",
       "last_return_date                  120.0  \n",
       "promised_date                       0.0  \n",
       "pickup_confirmation_date            0.0  \n",
       "delivered_date                     60.0  \n",
       "cancelled_date                    360.0  \n",
       "courier_onboarding_date             0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data={\n",
    "        'minimum times' : skimFrame[timeFeatures].min(),\n",
    "        'max times (since order)' : skimFrame[timeFeatures].max(),\n",
    "        'median times' : skimFrame[timeFeatures].median(),\n",
    "        'mean times' : skimFrame[timeFeatures].mean()\n",
    "    })\n",
    "\n",
    "# order_created_date was evidently a good choice since all the times are nonnegative.\n",
    "# all of these times appear to be very nearly normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlations with return flag</th>\n",
       "      <th>maximum correlation</th>\n",
       "      <th>miminum correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>last_active_date</th>\n",
       "      <td>-0.001169</td>\n",
       "      <td>0.810480</td>\n",
       "      <td>-0.005322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_purchase_date</th>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.703748</td>\n",
       "      <td>-0.006608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last_return_date</th>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.810480</td>\n",
       "      <td>-0.010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>promised_date</th>\n",
       "      <td>-0.002200</td>\n",
       "      <td>0.668709</td>\n",
       "      <td>-0.002200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pickup_confirmation_date</th>\n",
       "      <td>-0.003140</td>\n",
       "      <td>0.145246</td>\n",
       "      <td>-0.010394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delivered_date</th>\n",
       "      <td>-0.001349</td>\n",
       "      <td>0.820568</td>\n",
       "      <td>-0.001349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cancelled_date</th>\n",
       "      <td>-0.001037</td>\n",
       "      <td>0.820568</td>\n",
       "      <td>-0.001037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>courier_onboarding_date</th>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>-0.002975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>return_flag</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>-0.003140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          correlations with return flag  maximum correlation  \\\n",
       "last_active_date                              -0.001169             0.810480   \n",
       "last_purchase_date                             0.001878             0.703748   \n",
       "last_return_date                               0.001055             0.810480   \n",
       "promised_date                                 -0.002200             0.668709   \n",
       "pickup_confirmation_date                      -0.003140             0.145246   \n",
       "delivered_date                                -0.001349             0.820568   \n",
       "cancelled_date                                -0.001037             0.820568   \n",
       "courier_onboarding_date                        0.002916             0.002916   \n",
       "return_flag                                    0.000000             0.002916   \n",
       "\n",
       "                          miminum correlation  \n",
       "last_active_date                    -0.005322  \n",
       "last_purchase_date                  -0.006608  \n",
       "last_return_date                    -0.010394  \n",
       "promised_date                       -0.002200  \n",
       "pickup_confirmation_date            -0.010394  \n",
       "delivered_date                      -0.001349  \n",
       "cancelled_date                      -0.001037  \n",
       "courier_onboarding_date             -0.002975  \n",
       "return_flag                         -0.003140  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unhelpfulTimeFeatures = [\"rto_date\", \"rto_delivered_date\", 'reverse_logistics_booked_date', \n",
    "                        'reverse_logistics_date', 'reverse_logistics_delivered_date', \n",
    "                        'user_signup_date']\n",
    "\n",
    "# going to check for correlations amongst time features now.\n",
    "timeCorrelations = skimFrame[timeFeatures + ['return_flag']].corr()\n",
    "for i in range(9):\n",
    "\ttimeCorrelations.iloc[i,i] = 0.0\n",
    "relevantTimeCorrelations = pd.DataFrame(data={'miminum correlation' : timeCorrelations.min(), \n",
    "                                              'maximum correlation' : timeCorrelations.max(),\n",
    "                                              'correlations with return flag' : timeCorrelations['return_flag']})\n",
    "relevantTimeCorrelations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhMAAAFkCAYAAACabLnAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x104762ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 61320.,  21540.,  41640.,  32280.,   3540.,  53820.,  75000.,\n",
       "        86340.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# highest pairwise correlation is r = 0.820568... going to keep all of the features just to be safe. \n",
    "# none of the time features correlate well with return_flag... \n",
    "skimFrame = skimFrame.drop(unhelpfulTimeFeatures + ['order_created_date'], axis=1)\n",
    "skimFrame.to_csv(\"skimDataset2.csv\")\n",
    "newFeatures = ['last_active_date', 'last_purchase_date', 'last_return_date', 'promised_date',\n",
    " 'pickup_confirmation_date', 'delivered_date', 'cancelled_date', 'courier_onboarding_date']\n",
    "\n",
    "maxTimes = np.ones(len(newFeatures))\n",
    "for i in range(len(newFeatures)):\n",
    "    maxValue = skimFrame[newFeatures[i]].max()\n",
    "    maxTimes[i] = maxValue\n",
    "    quantFrame.loc[:,newFeatures[i]] = skimFrame[newFeatures[i]] / float(maxValue)\n",
    "\n",
    "x = np.asarray(quantFrame['pickup_confirmation_date'])\n",
    "y = np.asarray(quantFrame['return_flag'])\n",
    "plt.plot(x, y, 'o')\n",
    "plt.axis([0, 1, -0.5, 1.5])\n",
    "plt.show()\n",
    "# all of the time features will reveal something like this. \n",
    "\n",
    "maxTimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlations with return flag</th>\n",
       "      <th>maximum correlation</th>\n",
       "      <th>miminum correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>positive_time</th>\n",
       "      <td>0.003278</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>negative_time</th>\n",
       "      <td>-0.001532</td>\n",
       "      <td>0.144531</td>\n",
       "      <td>-0.001532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               correlations with return flag  maximum correlation  \\\n",
       "positive_time                       0.003278             0.144531   \n",
       "negative_time                      -0.001532             0.144531   \n",
       "\n",
       "               miminum correlation  \n",
       "positive_time             0.000000  \n",
       "negative_time            -0.001532  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'll take all of the time features negatively and positively correlated with returns and combine them respectively.\n",
    "df = skimFrame\n",
    "skimFrame.loc[:, \"negative_time\"] = df['last_active_date'].add(df['promised_date'].add(\n",
    "    df['pickup_confirmation_date'].add(df['delivered_date'].add(df['cancelled_date']))))\n",
    "skimFrame.loc[:, 'positive_time'] = df['last_purchase_date'].add(df['last_return_date'].add(\n",
    "    df['courier_onboarding_date']))\n",
    "\n",
    "localFrame = pd.DataFrame(data={'positive_time' : skimFrame['positive_time'],\n",
    "                               'negative_time' : skimFrame['negative_time'],\n",
    "                               'return_flag' : skimFrame['return_flag']})\n",
    "for i in range(2):\n",
    "    feature = localFrame.axes[1][i]\n",
    "    maxValue = localFrame[feature].max()\n",
    "    localFrame.loc[:,feature] = localFrame[feature] / float(maxValue)\n",
    "\n",
    "timeCorrelations = localFrame.corr()\n",
    "for i in range(3):\n",
    "    timeCorrelations.iloc[i,i] = 0.0\n",
    "relevantTimeCorrelations = pd.DataFrame(data={'miminum correlation' : timeCorrelations.min(), \n",
    "                                              'maximum correlation' : timeCorrelations.max(),\n",
    "                                              'correlations with return flag' : timeCorrelations['return_flag']})\n",
    "relevantTimeCorrelations.loc[['positive_time', 'negative_time'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Part 3*\n",
    "Here I worked with the qualitative features by examining their posterior probabilities to see if they were worth encoding into numerical features. In the training data, the rate of returns was 1.37%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# That experiment did not yield anything of value since those correlation values are not much better than anything before.\n",
    "# Hence I did not create positive_time, negative_time as features in quantFrame.\n",
    "\n",
    "# I'm going to look through the categorial features to see if it's worth it to do one-hot encoding for any of them. \n",
    "# In each of the groupby's, I check to see proportions of returns and see if there is signficant intercategorical variance.\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['payment_processor'],skimFrame['return_flag']]).count()\n",
    "# will be excluded. almost exactly uniform posterior probabilities, can be proxied by 'issuing_bank'\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['shipping_type'],skimFrame['return_flag']]).count() # interesting\n",
    "# Premium gets most returns (1.54%), Standard gets fewest (1.18%). \n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['issuing_bank'],skimFrame['return_flag']]).count() # interesting\n",
    "# Wells Fargo gets most returns (1.55%), followed by Citigroup (1.43%). Bank of America gets fewest (1.24%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['transaction_flag'],skimFrame['return_flag']]).count() # mildly interesting\n",
    "# Fraud gets most returns (1.51%), Expired gets fewest (1.25%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['product_category'],skimFrame['return_flag']]).count() # shorts?\n",
    "# Shorts gets most (1.71%), Sweater gets moderate (1.42%), Coat/Pants get least (1.29%, 1.24%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['product_sub_category'],skimFrame['return_flag']]).count()\n",
    "# Cotton/Nylon are higher than Polymer/Silk (~1.43% vs. ~1.31%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['product_features'],skimFrame['return_flag']]).count()\n",
    "# will be excluded. Hard to characterize, can be proxied by fit_score and product(_sub)_category\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['brand_name'],skimFrame['return_flag']]).count() # Raymond  vs. LeeCooper\n",
    "# LeeCooper gets most (1.52%), Raymond gets least (1.24%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['courier_name'],skimFrame['return_flag']]).count()\n",
    "# DHL gets slightly more than the rest (1.43% vs. ~1.34%)\n",
    "\n",
    "skimFrame['total_visits'].groupby([skimFrame['return_flag'],skimFrame['product_color']]).count()\n",
    "# will be excluded. Too sparsely scattered. \n",
    "\n",
    "skimFrame = skimFrame.drop(['payment_processor', 'product_features', 'product_color'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "shipping_type  return_flag\n",
       "Express        0              12240\n",
       "               1                171\n",
       "Premium        0              12376\n",
       "               1                194\n",
       "Same Day       0              12356\n",
       "               1                173\n",
       "Standard       0              12343\n",
       "               1                147\n",
       "Name: total_visits, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skimFrame['total_visits'].groupby([skimFrame['shipping_type'],skimFrame['return_flag']]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Part 4*\n",
    "I consolidated the functions I used to analyze the data so that I could use them quickly to work on the validation and test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepreprocess(original):\n",
    "    gender1 = lambda x : 1 if x == \"m\" else 0\n",
    "    gender2 = lambda x : 1 if x == \"Men\" else 0\n",
    "\n",
    "    original.loc[:, 'user_gender'] = original['user_gender'].apply(gender1)\n",
    "    original.loc[:, 'product_attribute_gender'] = original['product_attribute_gender'].apply(gender2)\n",
    "\n",
    "    # convert the one feature that has boolean values into zero-one\n",
    "    original.loc[:, 'fit_flag'] = original['fit_flag'].astype(int)\n",
    "\n",
    "    # create a new feature -> abs(fit_score) + fit_flag since fit_score seems to just be how off the fit is from ideal.\n",
    "    # Sometimes fit_score = 0 and fit_flag is True. We account for this by adding in the fit_flag value too. \n",
    "    original.loc[:, 'fit_score2'] = original['fit_score'].apply(abs) + original['fit_flag']\n",
    "    \n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The concise pipeline of converting timestamp data into useful, scaled data ready for the ML algorithm.\n",
    "# This will not be applied to the training set since I've done that above in steps that illustrate my thought process.\n",
    "\n",
    "def addNewFeatures(original, quantFrame):\n",
    "    timeFeatures = ['last_active_date', 'last_purchase_date', 'last_return_date', 'promised_date',\n",
    "     'pickup_confirmation_date', 'delivered_date', 'cancelled_date', 'courier_onboarding_date']\n",
    "    \n",
    "    scale = lambda x : x.seconds if hasattr(x, \"seconds\") else x # to avoid errors with NaT\n",
    "    clean = lambda x : None if x == -9223372036854775808 else x # after changing to int, NaT's become that number\n",
    "\n",
    "    for i in range(len(timeFeatures)):\n",
    "        newSeries = pd.to_datetime(original[timeFeatures[i]]) - pd.to_datetime(original['order_created_date'])\n",
    "        newSeries = newSeries.apply(scale).astype(int).apply(clean)\n",
    "        quantFrame.loc[:, timeFeatures[i]] = newSeries / maxTimes[i]\n",
    "    \n",
    "    return quantFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Another method to add new features to the dataFrame housing the data eventually meant for the ML algorithm.\n",
    "# First I will apply it to the training set, and later again to the validation and test sets.\n",
    "\n",
    "def addMoreNewFeatures(original, quantFrame):\n",
    "    premium_shipping = lambda x : 1 if x == \"Premium\" else 0\n",
    "    standard_shipping = lambda x : 1 if x == \"Standard\" else 0\n",
    "    wells_fargo = lambda x : 1 if x == \"Wells Fargo\" else 0\n",
    "    citigroup = lambda x : 1 if x == \"Citigroup\" else 0\n",
    "    bank_of_america = lambda x : 1 if x == \"Bank of America\" else 0\n",
    "    fraud = lambda x : 1 if x == \"Fraud\" else 0\n",
    "    expired = lambda x : 1 if x == \"Expired\" else 0\n",
    "    shorts = lambda x : 1 if x == \"Shorts\" else 0\n",
    "    sweater = lambda x : 1 if x == \"Sweater\" else 0\n",
    "    coatpants = lambda x : 1 if x == \"Coat\" or x == \"Pants\" else 0\n",
    "    cottonnylon = lambda x : 1 if x == \"Cotton\" or x == \"Nylon\" else 0\n",
    "    leecooper = lambda x : 1 if x == \"LeeCooper\" else 0\n",
    "    raymond = lambda x : 1 if x == \"Raymond\" else 0\n",
    "    dhl = lambda x : 1 if x == \"DHL\" else 0\n",
    "\n",
    "    quantFrame.loc[:, 'premium_shipping'] = original['shipping_type'].apply(premium_shipping)\n",
    "    quantFrame.loc[:, 'standard_shipping'] = original['shipping_type'].apply(standard_shipping)\n",
    "\n",
    "    quantFrame.loc[:, 'Wells_Fargo'] = original['issuing_bank'].apply(wells_fargo)\n",
    "    quantFrame.loc[:, 'Citigroup'] = original['issuing_bank'].apply(citigroup)\n",
    "    quantFrame.loc[:, 'Bank_of_America'] = original['issuing_bank'].apply(bank_of_america)\n",
    "\n",
    "    quantFrame.loc[:, 'fraud'] = original['transaction_flag'].apply(fraud)\n",
    "    quantFrame.loc[:, 'expired'] = original['transaction_flag'].apply(expired)\n",
    "\n",
    "    quantFrame.loc[:, 'shorts'] = original['product_category'].apply(shorts)\n",
    "    quantFrame.loc[:, 'sweater'] = original['product_category'].apply(sweater)\n",
    "    quantFrame.loc[:, 'coat/pants'] = original['product_category'].apply(coatpants)\n",
    "    \n",
    "    quantFrame.loc[:, 'cotton/nylon'] = original['product_sub_category'].apply(cottonnylon)\n",
    "\n",
    "    quantFrame.loc[:, 'LeeCooper'] = original['brand_name'].apply(leecooper)\n",
    "    quantFrame.loc[:, 'Raymond'] = original['brand_name'].apply(raymond)\n",
    "\n",
    "    quantFrame.loc[:, 'DHL'] = original['courier_name'].apply(dhl)\n",
    "    return quantFrame\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## *Part 5*\n",
    "I used the SVC class from the sklearn's SVM module, which uses the RBF kernel and a default C value of 1. I fit the classifier to the training data and then found a training accuracy of 98.63%. I wasn't really impressed since 98.63% of the data was in one class (not returned). Then I performed validation which gave me strange results. Afterward, I got a testing accuracy of 98.74%. Again, though, this was nearly exactly the divide in the data class-wise. This leads me to believe that the SVM was not an effective classification method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['total_visits', 'total_orders', 'last_purchase_value',\n",
      "       'high_value_customer', 'user_age', 'user_gender', 'total_order_value',\n",
      "       'total_item_count', 'product_price', 'product_discount',\n",
      "       'shipping_fees', 'transaction_fees', 'product_length',\n",
      "       'product_breadth', 'product_height', 'product_weight',\n",
      "       'product_attribute_gender', 'shipment_weight', 'heavy',\n",
      "       'delivery_attempt_count', 'user_rating', 'user_review_sentiment_score',\n",
      "       'return_flag', 'fit_flag', 'fit_score2', 'last_active_date',\n",
      "       'last_purchase_date', 'last_return_date', 'promised_date',\n",
      "       'pickup_confirmation_date', 'delivered_date', 'cancelled_date',\n",
      "       'courier_onboarding_date', 'premium_shipping', 'standard_shipping',\n",
      "       'Wells_Fargo', 'Citigroup', 'Bank_of_America', 'fraud', 'expired',\n",
      "       'shorts', 'sweater', 'coat/pants', 'cotton/nylon', 'LeeCooper',\n",
      "       'Raymond', 'DHL'],\n",
      "      dtype='object')\n",
      "(49708, 46)\n",
      "(49708,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98627987446688659"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "quantFrame = addMoreNewFeatures(skimFrame, quantFrame)\n",
    "cleanFrame = quantFrame.dropna()\n",
    "print(cleanFrame.axes[1])\n",
    "svmData = cleanFrame.drop(['return_flag'], axis=1).values\n",
    "svmTargets = cleanFrame['return_flag'].values\n",
    "\n",
    "print(svmData.shape)\n",
    "print(svmTargets.shape)\n",
    "classifier = SVC()\n",
    "classifier.fit(svmData, svmTargets)\n",
    "classifier.score(svmData, svmTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I'm going to perform validation now on 20000 entries. \n",
    "# The hsyperparameter I'll be adjusting is the \"C\" which is used in soft-margin SVM's. \n",
    "\n",
    "featuresToRemove = unhelpfulFeatures + redundantFeatures + unhelpfulTimeFeatures\n",
    "cvalues = [0.1, 0.2, 1/3, 0.5, 0.75, 1, 1.5, 2, 3, 5]\n",
    "for x in cvalues:\n",
    "    validationFrame = prepreprocess(pd.read_csv(\"Dataset.csv\").loc[:20000])\n",
    "    validationFrame = validationFrame.drop(featuresToRemove, axis=1)\n",
    "    validationQuantFrame = validationFrame.select_dtypes(exclude=['object'])\n",
    "    validationQuantFrame = addNewFeatures(validationFrame, validationQuantFrame)\n",
    "    validationQuantFrame = addMoreNewFeatures(validationFrame, validationQuantFrame)\n",
    "    \n",
    "    cleanValidationFrame = validationQuantFrame.dropna()\n",
    "    validationSVMData = cleanValidationFrame.drop(['return_flag'], axis=1).values\n",
    "    validationSVMTargets = cleanValidationFrame['return_flag'].values\n",
    "    classifier = SVC(C=x, probability=True, verbose=True)\n",
    "    classifier.fit(svmData, svmTargets) # refer to cell above - used for training set error.\n",
    "    score = classifier.score(validationSVMData, validationSVMTargets) * 100\n",
    "    print(\"Accuracy For C value of %s was %s\" % (x, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelkant/anaconda3/lib/python3.4/site-packages/pandas/core/indexing.py:297: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/neelkant/anaconda3/lib/python3.4/site-packages/pandas/core/indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set is 98.7373145326 percent\n"
     ]
    }
   ],
   "source": [
    "# Not sure why, but the validation accuracy was the same for all values of C. \n",
    "# Going to run the SVM on the test set now. \n",
    "featuresToRemove = unhelpfulFeatures + redundantFeatures + unhelpfulTimeFeatures\n",
    "testFrame = prepreprocess(pd.read_csv(\"Dataset.csv\").loc[20000:50000])\n",
    "testFrame = testFrame.drop(featuresToRemove, axis=1)\n",
    "testQuantFrame = testFrame.select_dtypes(exclude=['object'])\n",
    "testQuantFrame = addNewFeatures(testFrame, testQuantFrame)\n",
    "testQuantFrame = addMoreNewFeatures(testFrame, testQuantFrame)\n",
    "testQuantFrame.axes[1]\n",
    "\n",
    "cleanTestFrame = testQuantFrame.dropna()\n",
    "testsvmData = cleanTestFrame.drop(['return_flag'], axis=1).values\n",
    "testsvmTargets = cleanTestFrame['return_flag'].values\n",
    "testScore = classifier.score(testsvmData, testsvmTargets) * 100\n",
    "print(\"Accuracy on test set is %s percent\" % (testScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "return_flag\n",
       "0    29622\n",
       "1      379\n",
       "Name: fit_flag, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnTest = pd.read_csv(\"Dataset.csv\").loc[20000:50000]\n",
    "returnTest['fit_flag'].groupby(returnTest['return_flag']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## *Part 6*\n",
    "Dissatisfied with the SVM results, I went back to the drawing board by taking all of the quantitative features and feeding them through a SVD algorithm to see if there was a better way to featurize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98627987446688659"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "featureMaker = TruncatedSVD(n_components=20)\n",
    "svdTraining = featureMaker.fit_transform(svmData, svmTargets) # cleanFrame - cleaned out quantFrame (beginning of part 5).\n",
    "svdClassifier = SVC()\n",
    "svdClassifier.fit(svdTraining, svmTargets)\n",
    "svdClassifier.score(svdTraining, svmTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02079959,  0.07905295,  0.05300763,  0.05227007,  0.05148185,\n",
       "        0.05125846,  0.04997242,  0.04877019,  0.04143261,  0.03974792,\n",
       "        0.03902474,  0.03843673,  0.03585154,  0.03458227,  0.03326223,\n",
       "        0.0260064 ,  0.02559632,  0.02519793,  0.02421171,  0.0211394 ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I got exactly the same training accuracy even with SVD to help out... \n",
    "featureMaker.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98737314532605425"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seems like a pretty solid balance between the different features.\n",
    "svdClassifier.score(featureMaker.transform(testsvmData), testsvmTargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Got exactly the same test accuracy... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Conclusion*\n",
    "Since I got exactly the same training/test accuracy with just the regular features and the SVD transform, I'm comfortable saying that the issue is more likely in the choice of model (SVM) than the featurization. I think decision trees would be the next model I would explore. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
